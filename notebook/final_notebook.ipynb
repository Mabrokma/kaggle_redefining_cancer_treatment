{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + GRU + bidirectional + Attentional context\n",
    "\n",
    "This kernes uses a recurrent neural network in keras that uses GRU cells with a bidirectional layer and an attention context layer. The model uses the begining of the text and the end of the text and them join both outputs along with an one hot encoded layer for the gene and another for the variation. The variation has been encoded using the first and the last letter.\n",
    "\n",
    "This kernel is based in a kernel by [ReiiNakanoBasic](https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "ef08df1c-3c94-4f63-bbc7-6cc93f46fb52",
    "_execution_state": "idle",
    "_uuid": "7b62b6d64cc54d675b18c29ee12eed7cd45a3154"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gensim\n",
    "\n",
    "import scikitplot.plotters as skplt\n",
    "\n",
    "import nltk\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0b39f26e-9c4c-46db-8fe5-86967734c0e5",
    "_uuid": "58b74d086e6f6f067f337470219be9fd43211c08"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "011dde04-0160-41cd-9149-5aeac26295fb",
    "_execution_state": "idle",
    "_uuid": "1b07b556871714b93d70806d58b5225be507e716",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/companies/chute/pythonenv/iris3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/jorge/companies/chute/pythonenv/iris3/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/jorge/companies/chute/pythonenv/iris3/lib/python3.6/site-packages/ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_train_txt = pd.read_csv('../input/training_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "df_train_var = pd.read_csv('../input/training_variants')\n",
    "df_val_txt = pd.read_csv('../input/test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "df_val_var = pd.read_csv('../input/test_variants')\n",
    "\n",
    "df_test_txt = pd.read_csv('../input/stage2_test_text.csv', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "df_test_var = pd.read_csv('../input/stage2_test_variants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/companies/chute/pythonenv/iris3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_val_txt = pd.read_csv('../input/test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "df_val_var = pd.read_csv('../input/test_variants')\n",
    "df_val_labels = pd.read_csv('../input/stage1_solution_filtered.csv')\n",
    "df_val_labels['Class'] = pd.to_numeric(df_val_labels.drop('ID', axis=1).idxmax(axis=1).str[5:])\n",
    "df_val_labels = df_val_labels[['ID', 'Class']]\n",
    "df_val_txt = pd.merge(df_val_txt, df_val_labels, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>TET2</td>\n",
       "      <td>Y1902A</td>\n",
       "      <td>TET proteins oxidize 5-methylcytosine (5mC) on...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>MTOR</td>\n",
       "      <td>D2512H</td>\n",
       "      <td>Genes encoding components of the PI3K-Akt-mTOR...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>KIT</td>\n",
       "      <td>D52N</td>\n",
       "      <td>Myeloproliferative disorders (MPD) constitute ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>SPOP</td>\n",
       "      <td>F125V</td>\n",
       "      <td>In the largest E3 ligase subfamily, Cul3 binds...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>KEAP1</td>\n",
       "      <td>C23Y</td>\n",
       "      <td>Keap1 is the substrate recognition module of a...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID   Gene Variation                                               Text  \\\n",
       "12  12   TET2    Y1902A  TET proteins oxidize 5-methylcytosine (5mC) on...   \n",
       "19  19   MTOR    D2512H  Genes encoding components of the PI3K-Akt-mTOR...   \n",
       "21  21    KIT      D52N  Myeloproliferative disorders (MPD) constitute ...   \n",
       "55  55   SPOP     F125V  In the largest E3 ligase subfamily, Cul3 binds...   \n",
       "64  64  KEAP1      C23Y  Keap1 is the substrate recognition module of a...   \n",
       "\n",
       "    Class  \n",
       "12    1.0  \n",
       "19    2.0  \n",
       "21    2.0  \n",
       "55    4.0  \n",
       "64    4.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.merge(df_train_var, df_train_txt, how='left', on='ID')\n",
    "df_train.head()\n",
    "df_test = pd.merge(df_test_var, df_test_txt, how='left', on='ID')\n",
    "df_test.head()\n",
    "df_val = pd.merge(df_val_var, df_val_txt, how='left', on='ID')\n",
    "df_val = df_val[df_val_txt['Class'].notnull()]\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f6abf33d-337d-4be8-8d50-34d2292a4c71",
    "_uuid": "525e0ac244f14d7dbc0edfe5783c1439a3a10d3d"
   },
   "source": [
    "## Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "609177df-1799-4f5e-b3e7-77c820561a7c",
    "_execution_state": "busy",
    "_uuid": "d631c05ece2e126a82481fa5a262d12ec7577e38"
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=8)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "e62cf926-34d9-461a-bf92-21e38e9ff2d3",
    "_execution_state": "busy",
    "_uuid": "80d89a6184209c1630976aa5bdb9a93b1290b363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found w2vmodel\n"
     ]
    }
   ],
   "source": [
    "w2vec = get_word2vec(\n",
    "    MySentences(\n",
    "        df_train['Text'].values, \n",
    "        df_val['Text'].values\n",
    "    ),\n",
    "    'w2vmodel'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "591cc8cb-49c2-41d0-be0a-ff8ee7d4f230",
    "_uuid": "b41f276a240b388d56fb5187a5b99ac0d3b76d59"
   },
   "source": [
    "### Tokenizer\n",
    "We'll define a transformer (with sklearn interface) to convert a document into its corresponding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "8be99804-9357-4522-a80b-aa328f5ef973",
    "_execution_state": "busy",
    "_uuid": "f188944da320461ad2e8f76a7a991454e2c0763c"
   },
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7db18b6e-d23f-412a-9f16-429082572649",
    "_uuid": "934ddee5cd73ca17ee2bd73f647290697aca997c"
   },
   "source": [
    "## RNN in Keras\n",
    "We use a vocabulary of 10000 most used words and a sequence length of 3000 words (3000 for the beggining and 3000 for the ending)\n",
    "\n",
    "This takes about few hours to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "d1b0a192-ea18-419e-9ba4-bb52f12b9f78",
    "_execution_state": "busy",
    "_uuid": "8d4ee34e1e95faa8be9d93ecced116c324b61465"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Use the Keras tokenizer\n",
    "VOCABULARY_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 3000\n",
    "tokenizer = Tokenizer(num_words=VOCABULARY_SIZE)\n",
    "tokenizer.fit_on_texts(df_train['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "0610df13-711b-4d28-810a-2f7361cea388",
    "_execution_state": "busy",
    "_uuid": "2fba830b1fd73426d1b3b2f22bac503d91d0a923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3321, 3000) (3321, 3000) (3321, 9)\n"
     ]
    }
   ],
   "source": [
    "# Train set\n",
    "train_set = df_train.sample(frac=1) # shuffle data first\n",
    "train_set_input = tokenizer.texts_to_sequences(train_set['Text'].values)\n",
    "train_set_input_reverse = [list(reversed(x)) for x in train_set_input]\n",
    "train_set_input_begin = pad_sequences(train_set_input, maxlen=SEQUENCE_LENGTH)\n",
    "train_set_input_end = pad_sequences(train_set_input_reverse, maxlen=SEQUENCE_LENGTH)\n",
    "train_set_output = pd.get_dummies(train_set['Class']).values\n",
    "print(train_set_input_begin.shape, train_set_input_end.shape, train_set_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "0610df13-711b-4d28-810a-2f7361cea388",
    "_execution_state": "busy",
    "_uuid": "2fba830b1fd73426d1b3b2f22bac503d91d0a923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368, 3000) (368, 3000) (368, 9)\n"
     ]
    }
   ],
   "source": [
    "# Validation set\n",
    "val_set_input = tokenizer.texts_to_sequences(df_val['Text'].values)\n",
    "val_set_input_reverse = [list(reversed(x)) for x in val_set_input]\n",
    "val_set_input_begin = pad_sequences(val_set_input, maxlen=SEQUENCE_LENGTH)\n",
    "val_set_input_end = pad_sequences(val_set_input_reverse, maxlen=SEQUENCE_LENGTH)\n",
    "val_set_output = pd.get_dummies(df_val['Class']).values\n",
    "print(val_set_input_begin.shape, val_set_input_end.shape, val_set_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "0610df13-711b-4d28-810a-2f7361cea388",
    "_execution_state": "busy",
    "_uuid": "2fba830b1fd73426d1b3b2f22bac503d91d0a923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(986, 3000) (986, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "test_set_input = tokenizer.texts_to_sequences(df_test['Text'].values)\n",
    "test_set_input_reverse = [list(reversed(x)) for x in test_set_input]\n",
    "test_set_input_begin = pad_sequences(test_set_input, maxlen=SEQUENCE_LENGTH)\n",
    "test_set_input_end = pad_sequences(test_set_input_reverse, maxlen=SEQUENCE_LENGTH)\n",
    "print(test_set_input_begin.shape, test_set_input_end.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add genes and variations as one hot encoding\n",
    "We only transform the variations to use the first and last letter, otherwise it will be almos one variation per exmple and it will be useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "0610df13-711b-4d28-810a-2f7361cea388",
    "_execution_state": "busy",
    "_uuid": "2fba830b1fd73426d1b3b2f22bac503d91d0a923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique genes:  401\n",
      "Unique variations: 341\n",
      "3321 3321\n",
      "368 368\n",
      "986 986\n"
     ]
    }
   ],
   "source": [
    "# Add gene and variation to predictor\n",
    "gene_le = LabelEncoder()\n",
    "all_genes = np.concatenate([df_train['Gene'], df_val['Gene'], df_test['Gene']])\n",
    "all_variations = np.concatenate([df_train['Variation'], df_val['Variation'], df_test['Variation']])\n",
    "all_variations = np.asarray([v[0]+v[-1] for v in all_variations])\n",
    "print (\"Unique genes: \", len(np.unique(all_genes)))\n",
    "print (\"Unique variations:\", len(np.unique(all_variations)))\n",
    "\n",
    "# gene_encoded = gene_le.fit_transform(all_genes.ravel()).reshape(-1, 1)\n",
    "# gene_encoded = gene_encoded / np.max(gene_encoded.ravel())\n",
    "# variation_le = LabelEncoder()\n",
    "# variation_encoded = variation_le.fit_transform(all_variations).reshape(-1, 1)\n",
    "# variation_encoded = variation_encoded / np.max(variation_encoded)\n",
    "\n",
    "gene_encoded = pd.get_dummies(all_genes).values\n",
    "variation_encoded = pd.get_dummies(all_variations).values\n",
    "\n",
    "len_train_set = len(train_set_input)\n",
    "len_val_set = len(val_set_input)\n",
    "len_test_set = len(test_set_input)\n",
    "train_set_input_gene = gene_encoded[:len_train_set]\n",
    "train_set_input_variation = variation_encoded[:len_train_set]\n",
    "val_set_input_gene = gene_encoded[len_train_set:-len_test_set]\n",
    "val_set_input_variation = variation_encoded[len_train_set:-len_test_set]\n",
    "test_set_input_gene = gene_encoded[-len_test_set:]\n",
    "test_set_input_variation = variation_encoded[-len_test_set:]\n",
    "\n",
    "print (len_train_set, len(train_set_input_gene))\n",
    "print (len_val_set, len(val_set_input_gene))\n",
    "print (len_test_set, len(test_set_input_gene))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention layer\n",
    "\n",
    "from: https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "import numpy as np\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "af64e666-69b4-4fe7-a8ce-c098e640cbf9",
    "_execution_state": "busy",
    "_uuid": "6aff6b56a17d5bc0650e1bb41e60fee7be6f52ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 3000, 128)     1280000     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 3000, 128)     1280000     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 401)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 341)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 3000, 392)     382200      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 3000, 392)     382200      embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 742)           0           input_3[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attention_with_context_1 (Attent (None, 392)           154448      bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "attention_with_context_2 (Attent (None, 392)           154448      bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 32)            23776       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 816)           0           attention_with_context_1[0][0]   \n",
      "                                                                   attention_with_context_2[0][0]   \n",
      "                                                                   dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 9)             7353        concatenate_2[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 3,664,425\n",
      "Trainable params: 3,664,425\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Bidirectional, Merge, Input, concatenate\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "# Build out our simple LSTM\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "# Model saving callback\n",
    "ckpt_callback = ModelCheckpoint('keras_model', \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='auto')\n",
    "\n",
    "\n",
    "input_sequence_begin = Input(shape=(train_set_input_begin.shape[1],))\n",
    "input_sequence_end = Input(shape=(train_set_input_end.shape[1],))\n",
    "input_gene = Input(shape=(train_set_input_gene.shape[1],))\n",
    "input_variant = Input(shape=(train_set_input_variation.shape[1],))\n",
    "\n",
    "merged = concatenate([input_gene, input_variant])\n",
    "dense = Dense(32, activation='sigmoid')(merged)\n",
    "\n",
    "embeds_begin = Embedding(VOCABULARY_SIZE, embed_dim, input_length = SEQUENCE_LENGTH)(input_sequence_begin)\n",
    "embeds_out_begin = Bidirectional(GRU(lstm_out, recurrent_dropout=0.2, dropout=0.2, return_sequences=True))(embeds_begin)\n",
    "attention_begin = AttentionWithContext()(embeds_out_begin)\n",
    "\n",
    "embeds_end = Embedding(VOCABULARY_SIZE, embed_dim, input_length = SEQUENCE_LENGTH)(input_sequence_end)\n",
    "embeds_out_end = Bidirectional(GRU(lstm_out, recurrent_dropout=0.2, dropout=0.2, return_sequences=True))(embeds_end)\n",
    "attention_end = AttentionWithContext()(embeds_out_end)\n",
    "\n",
    "merged2 = concatenate([attention_begin, attention_end, dense])\n",
    "dense2 = Dense(9,activation='softmax')(merged2)\n",
    "\n",
    "model = Model(inputs=[input_sequence_begin, input_sequence_end, input_gene, input_variant], outputs=dense2)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "d7eac40d-a8d6-47a7-9542-1a2b06f76ca8",
    "_execution_state": "busy",
    "_uuid": "a0da0f4a43684380e3dd1b6f53d03cc45384fbf9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3321 samples, validate on 368 samples\n",
      "Epoch 1/6\n",
      "3312/3321 [============================>.] - ETA: 8s - loss: 1.5824 Epoch 00000: val_loss improved from 2.17030 to 1.20471, saving model to keras_model\n",
      "3321/3321 [==============================] - 3139s - loss: 1.5810 - val_loss: 1.2047\n",
      "Epoch 2/6\n",
      "3312/3321 [============================>.] - ETA: 6s - loss: 1.2553 Epoch 00001: val_loss did not improve\n",
      "3321/3321 [==============================] - 2399s - loss: 1.2536 - val_loss: 1.2269\n",
      "Epoch 3/6\n",
      "3312/3321 [============================>.] - ETA: 6s - loss: 1.0623 Epoch 00002: val_loss improved from 1.20471 to 1.18388, saving model to keras_model\n",
      "3321/3321 [==============================] - 2314s - loss: 1.0612 - val_loss: 1.1839\n",
      "Epoch 4/6\n",
      "3312/3321 [============================>.] - ETA: 6s - loss: 0.9026 Epoch 00003: val_loss improved from 1.18388 to 1.11628, saving model to keras_model\n",
      "3321/3321 [==============================] - 2314s - loss: 0.9043 - val_loss: 1.1163\n",
      "Epoch 5/6\n",
      "3312/3321 [============================>.] - ETA: 6s - loss: 0.7617 Epoch 00004: val_loss improved from 1.11628 to 1.08450, saving model to keras_model\n",
      "3321/3321 [==============================] - 2302s - loss: 0.7623 - val_loss: 1.0845\n",
      "Epoch 6/6\n",
      "3312/3321 [============================>.] - ETA: 6s - loss: 0.6638 Epoch 00005: val_loss did not improve\n",
      "3321/3321 [==============================] - 2280s - loss: 0.6645 - val_loss: 1.0865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1372c242b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_set_input_begin, train_set_input_end, train_set_input_gene, train_set_input_variation], train_set_output, \n",
    "          epochs=6, batch_size=16, \n",
    "          validation_data=([val_set_input_begin,val_set_input_end,val_set_input_gene,val_set_input_variation], val_set_output), \n",
    "          callbacks=[ckpt_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "72101f81-dd5a-4b27-b985-e4fa03a7d7fd",
    "_execution_state": "busy",
    "_uuid": "142200cc3855cd276bb3a9abf3526a61988fafee"
   },
   "outputs": [],
   "source": [
    "model = load_model('keras_model', custom_objects={'AttentionWithContext': AttentionWithContext})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "da8194c1-bce5-4d3d-93a7-fbb6b832113e",
    "_execution_state": "busy",
    "_uuid": "fffe9f6d9907deabbf827f183c4cf33af917b131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 1.084504352362748\n",
      "Accuracy: 0.6059782608695652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/companies/chute/pythonenv/iris3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:75: DeprecationWarning: Function plot_confusion_matrix is deprecated; This will be removed in v0.4.0. Please use scikitplot.metrics.plot_confusion_matrix instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f13659d0eb8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATIAAAEWCAYAAADl+xvlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13728f7be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probas = model.predict([val_set_input_begin, val_set_input_end, val_set_input_gene, val_set_input_variation])\n",
    "pred_indices = np.argmax(probas, axis=1)\n",
    "classes = np.array(range(1, 10))\n",
    "preds = classes[pred_indices]\n",
    "print('Log loss: {}'.format(log_loss(classes[np.argmax(val_set_output, axis=1)], probas)))\n",
    "print('Accuracy: {}'.format(accuracy_score(classes[np.argmax(val_set_output, axis=1)], preds)))\n",
    "skplt.plot_confusion_matrix(classes[np.argmax(val_set_output, axis=1)], preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with validation set\n",
    "\n",
    "For the final submission we can add the validation set to the training set and run the network for 4 epochs. After 4 epcohs it starts overfitting in the validation set. We only do this to add more training samples and try to get better results this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3689/3689 [==============================] - 2567s - loss: 0.7371  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/companies/chute/pythonenv/iris3/lib/python3.6/site-packages/keras/callbacks.py:405: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/4\n",
      "3689/3689 [==============================] - 2568s - loss: 0.6288  \n",
      "Epoch 3/4\n",
      "3689/3689 [==============================] - 2557s - loss: 0.5720  \n",
      "Epoch 4/4\n",
      "3689/3689 [==============================] - 2572s - loss: 0.6104  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1366523748>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([\n",
    "    np.concatenate([train_set_input_begin, val_set_input_begin]), \n",
    "    np.concatenate([train_set_input_end,val_set_input_end]), \n",
    "    np.concatenate([train_set_input_gene, val_set_input_gene]), \n",
    "    np.concatenate([train_set_input_variation, val_set_input_variation])\n",
    "],  np.concatenate([train_set_output,val_set_output]), \n",
    "    epochs=4, batch_size=16, callbacks=[ckpt_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e70c3e0c-3722-4ed0-943f-3909c2078f38",
    "_uuid": "2eeb0f102a078b433d0c30b9d4e5fedebc58a236"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "4d608be7-741c-462c-8a2a-39b81612000d",
    "_execution_state": "busy",
    "_uuid": "7b454ebc7996841cffcaba3a165d8b28b5c76466"
   },
   "outputs": [],
   "source": [
    "probas = model.predict([test_set_input_begin, test_set_input_end, test_set_input_gene, test_set_input_variation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "dbcb71f9-1854-44c3-b9cf-d060494887d8",
    "_execution_state": "busy",
    "_uuid": "81165d6e9870765c90fbb4624743ea49f9f6d1d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>class4</th>\n",
       "      <th>class5</th>\n",
       "      <th>class6</th>\n",
       "      <th>class7</th>\n",
       "      <th>class8</th>\n",
       "      <th>class9</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.238091</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>0.668008</td>\n",
       "      <td>0.032579</td>\n",
       "      <td>0.028462</td>\n",
       "      <td>0.020158</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.623407</td>\n",
       "      <td>0.036642</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.183017</td>\n",
       "      <td>0.055994</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.020672</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>0.003614</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007038</td>\n",
       "      <td>0.332710</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.008286</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>0.642188</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.420275</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.570505</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034064</td>\n",
       "      <td>0.092081</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.014544</td>\n",
       "      <td>0.012756</td>\n",
       "      <td>0.014006</td>\n",
       "      <td>0.828419</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class1    class2    class3    class4    class5    class6    class7  \\\n",
       "0  0.238091  0.007231  0.004479  0.668008  0.032579  0.028462  0.020158   \n",
       "1  0.623407  0.036642  0.001584  0.183017  0.055994  0.072700  0.020672   \n",
       "2  0.007038  0.332710  0.001060  0.000995  0.008286  0.006846  0.642188   \n",
       "3  0.001037  0.420275  0.000757  0.000485  0.003372  0.002968  0.570505   \n",
       "4  0.034064  0.092081  0.001345  0.014544  0.012756  0.014006  0.828419   \n",
       "\n",
       "     class8    class9  ID  \n",
       "0  0.000361  0.000631   1  \n",
       "1  0.002371  0.003614   2  \n",
       "2  0.000697  0.000178   3  \n",
       "3  0.000505  0.000096   4  \n",
       "4  0.001950  0.000836   5  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])\n",
    "submission_df['ID'] = df_test['ID']\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "9ce11af2-21b8-47e0-a83b-55ee14c1c53c",
    "_execution_state": "busy",
    "_uuid": "7264bb21f9c0028b6be3e637c8414d543266d113"
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1aa03cb-3faf-4dfe-97a7-97dbfff856ec",
    "_uuid": "1ac56c47c3e55b3a6fde88332fe1e045e59fef9b"
   },
   "source": [
    "# Public LB Score: 0.93662\n",
    "\n",
    "The private leaderboard shows and score of 2.8. Everybody get much worse results in the private leader board, there has been a long discussion in the forums."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
